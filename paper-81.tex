
% -----------------------------------
% paper # 81
% -----------------------------------

Leofante, Vuotto, \'{A}brah\'{a}m, Tacchella and Jansen
\cite{isola-2016-leofante}
({\em Combining Static and Runtime Methods to Achieve 
      Safe Standing-Up for Humanoid Robots})
address how to improve a scripted stand up strategy for robots, making
it safe and stable, using a combination of RV and SV. 
This paper describes a novel approach to achieve safe standing-up for humanoid robots. It proposes a combination of three methods. The first is reinforcement learning that uses Q-learning based on a robot simulator to construct a standing-up strategy. The second method is greedy model repair that uses efficient probabilistic model checkers to repair the strategy to avoid given unsafe states with a given probabilistic threshold. These two methods result in an initial strategy that is deployed on the robot. As the strategy has been obtained on an idealized model of the real robot and environment, it may still not be adequate. Therefore, the third method is runtime monitoring with a feedback loop to observe the real-time behaviour of the robot and adapt the strategy on the go. 
The implementation of the presented theory is
ongoing, but already some experimental results for (model free)
reinforcement learning strategies are presented.

